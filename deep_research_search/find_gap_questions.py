"""Module use to find the gap questions that will lead to the search of the gap knowledge required to answer a specific question."""
import json

from deep_research_search.utils import query_ollama
from deep_research_search.output_formats import GapQuestionsGenerationOutputFormat

def find_gap_questions(memory, question_to_answer):
    """
    Identifies gaps in knowledge using a local LLM (via OLlama) and returns gap questions.

    Args:
        memory (dict): The current in-memory knowledge state containing at least:
                       - "knowledge": a list of gathered knowledge items.
        question_to_answer (str): The actual question to use to find the gap knowledge that will be used to answer it.

    Returns:
        list: A list of gap questions (strings) generated by the LLM.
              If no gaps are found or an error occurs, returns an empty list.

    Role:
        This function builds a prompt using the initial query and the aggregated text from all 
        collected knowledge. It then queries the LLM (via the query_ollama function) to identify
        any missing information and formulate follow-up (gap) questions.
    """
    # Aggregate all collected knowledge text.
    aggregated_text = " ".join([item["text"] for item in memory.get("knowledge", [])])
    
    # Build the prompt for the LLM.
    prompt = (
        f"You are an AI assistant specialized in identifying missing information gaps to answer a specific question.\n"
        f"The initial question is: \"{question_to_answer}\"\n"
        f"The following knowledge has been collected so far:\n{aggregated_text}\n\n"
        "Based on this information, list 3 follow-up questions (gap questions) that need to be answered to fill in the missing details."
        "These questions will be used as web search requests to find some web pages that contains interesting informations so keep this in mind to generate efficient questions." 
        "Respond with a JSON that include a \"gap_questions\" key. The value of this key need to be a list of strings where the strings are the different questiosn generated."
        "Generate only the JSON in your final answer and nothing else."
    )
    
    # Query the LLM using OLlama.
    try:
        response = query_ollama(prompt=prompt, output_format=GapQuestionsGenerationOutputFormat)
        # The response is expected to be a JSON array of strings.
        if isinstance(response, str):
            gap_questions = list(json.loads(response).get("gap_questions", []))
        else:
            gap_questions = []
    except Exception as e:
        print(f"Error querying gap questions: {e}")
        gap_questions = []

    return gap_questions